---
title: "Validating submissions locally"
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(hubValidations)
```

## Validating model output files with `validate_submission()`

While most hubs will have automated validation systems set up to check contributions during submission, `hubValidations` also provides functionality for validating files locally before submitting them.

For this, **submitting teams can use `validate_submission()` to validate their model output files prior to submitting.**

The function takes a relative path, relative to the model output directory, as argument `file_path`, performs a series of standard validation checks and returns their results in the form of a `hub_validations` S3 class object.

```{r}
hub_path <- system.file("testhubs/simple", package = "hubValidations")

validate_submission(hub_path,
  file_path = "team1-goodmodel/2022-10-08-team1-goodmodel.csv"
)
```

For more details on the structure of `<hub_validations>` objects, including how to access more information on individual checks, see `vignette("articles/hub-validations-class")`.

### Validation early return

**Some checks which are critical to downstream checks will cause validation to stop and return the results of the checks up to and including the critical check that failed early.**

They generally return a `<error/check_error>` condition class object.
Any problems identified will need to be resolved and the function re-run for validation to proceed further.


```{r}
hub_path <- system.file("testhubs/simple", package = "hubValidations")

validate_submission(hub_path,
  file_path = "team1-goodmodel/2022-10-15-hub-baseline.csv"
)
```

### Execution Errors

If an execution error occurs in any of the checks, an `<error/check_exec_error>` is returned instead. For validation purposes, this results in the same downstream effects as an `<error/check_error>` object.


### Checking for errors with `check_for_errors()`

You can check whether your file will overall pass validation checks by passing the `hub_validations` object to `check_for_errors()`. 

If validation fails, an error will be thrown and the failing checks will be summarised.

```{r, error=TRUE}
validate_submission(hub_path,
  file_path = "team1-goodmodel/2022-10-08-team1-goodmodel.csv"
) %>%
  check_for_errors()
```



### Skipping the submission window check

If you are preparing your submission prior to the submission window opening, you might want to skip the submission window check.
You can so by setting argument `skip_submit_window_check` to `TRUE`. 

This results in the previous valid file (except for failing the validation window check) now passing overall validation.

```{r}
validate_submission(hub_path,
  file_path = "team1-goodmodel/2022-10-08-team1-goodmodel.csv",
  skip_submit_window_check = TRUE
) %>%
  check_for_errors()
```

### Ignoring derived task IDs to improve validation performance

#### What are derived task IDs?

Derived task IDs are a class of task ID whose values depend on the values of other task IDs. As such, the **validity of derived task ID values is dependent on the values of the task IDs they are derived from** and the validity of value combinations of derived and other task IDs is much more restricted. A common example of a derived task ID is `target_end_date` which is most often derived from the `reference_date` or `origin_date` and `horizon` task ids. 

#### How to ignore derived task IDs

<div class="alert alert-info" role="note">

**For configs using schema version v4.0.0 and above, derived task IDs are configured via the hub config and do not need to be ignored manually**

To check if the hub uses schema version v4.0.0 or above, you can use:
```r
hubUtils::version_gte("v4.0.0", hub_path = "path/to/hub")
```

</div>

Argument **`derived_task_ids`** allows for the specification of **task IDs that are derived from other task IDs**. Supplying the names of derived task IDs to argument `derived_task_ids` will ignore them during validation checks.

Depending on config complexity, this **can often lead to a significant improvement in validation performance and in some circumstances is necessary for correct validation**.

<div class="alert alert-warning" role="note">

Note that, **if any task IDs with `required` values have dependent derived task IDs, it is essential for `derived_task_ids` to be specified**. 
If this is the case and derived task IDs are not specified, the dependent nature of derived task ID values will result in **false validation errors when validating required values**.

If you're unsure if this is the case for your hub or which task IDs are derived, please consult the hub documentation or contact the hub administrators.

</div>

### `validate_submission` check details

```{r, echo=FALSE}
library(kableExtra)
arrow::read_csv_arrow(system.file("check_table.csv", package = "hubValidations")) %>%
  dplyr::filter(
    .data$`parent fun` != "validate_model_metadata",
    !.data$optional
  ) %>%
  dplyr::select(-"parent fun", -"check fun", -"optional") %>%
  dplyr::mutate("Extra info" = dplyr::case_when(
    is.na(.data$`Extra info`) ~ "",
    TRUE ~ .data$`Extra info`
  )) %>%
  knitr::kable(caption = "Details of checks performed by `validate_submission()`") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>%
  column_spec(1, bold = TRUE)
```


## Validating model metadata files with `validate_model_metadata()`

If you want to check a model metadata file before submitting, you can similarly use function `validate_model_metadata()`.

The function takes model metadata file name as argument `file_path`, performs a series of validation checks and returns their results in the form of a `hub_validations` S3 class object.

```{r}
validate_model_metadata(hub_path,
  file_path = "hub-baseline.yml"
)

validate_model_metadata(hub_path,
  file_path = "team1-goodmodel.yaml"
)
```


For more details on the structure of `<hub_validations>` objects, including how to access more information on individual checks, see `vignette("articles/hub-validations-class")`.

### `validate_model_metadata` check details

```{r, echo=FALSE}
library(kableExtra)
arrow::read_csv_arrow(system.file("check_table.csv", package = "hubValidations")) %>%
  dplyr::filter(
    .data$`parent fun` == "validate_model_metadata",
    !.data$optional
  ) %>%
  dplyr::select(-"parent fun", -"check fun", -"optional") %>%
  dplyr::mutate("Extra info" = dplyr::case_when(
    is.na(.data$`Extra info`) ~ "",
    TRUE ~ .data$`Extra info`
  )) %>%
  knitr::kable(caption = "Details of checks performed by `validate_model_metadata()`") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>%
  column_spec(1, bold = TRUE)
```


<div class="alert alert-info" role="note">

#### Custom checks

The standard checks discussed here are the checks deployed by default by the `validate_submission` or `validate_model_metadata` functions. 
For more information on deploying optional/custom functions or functions that require configuration please check the article on [including custom functions](deploying-custom-functions.html) (`vignette("articles/deploying-custom-functions")`).

</div>
