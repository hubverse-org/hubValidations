% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/check_target_tbl_coltypes.R
\name{check_target_tbl_coltypes}
\alias{check_target_tbl_coltypes}
\title{Check that a target data file has the correct column types according to
target type}
\usage{
check_target_tbl_coltypes(
  target_tbl,
  target_type = c("time-series", "oracle-output"),
  date_col = NULL,
  na = c("NA", ""),
  output_type_id_datatype = c("from_config", "auto", "character", "double", "integer",
    "logical", "Date"),
  file_path,
  hub_path
)
}
\arguments{
\item{target_tbl}{A tibble/data.frame of the contents of the target data file
being validated.}

\item{target_type}{Type of target data to retrieve matching files. One of "time-series" or
"oracle-output". Defaults to "time-series".}

\item{date_col}{Optional column name to be interpreted as date. Default is \code{NULL}.
Useful when the required date column is a partitioning column in the target data
and does not have the same name as a date typed task ID variable in the config.}

\item{na}{A character vector of strings to interpret as missing values. Only
applies to CSV files. The default is \code{c("NA", "")}. Useful when actual character
string \code{"NA"} values are used in the data. In such a case, use empty cells to
indicate missing values in your files and set \code{na = ""}.}

\item{output_type_id_datatype}{character string. One of \code{"from_config"}, \code{"auto"},
\code{"character"}, \code{"double"}, \code{"integer"}, \code{"logical"}, \code{"Date"}.
Defaults to \code{"from_config"} which uses the setting in the \code{output_type_id_datatype}
property in the \code{tasks.json} config file if available. If the property is
not set in the config, the argument falls back to \code{"auto"} which determines
the  \code{output_type_id} data type automatically from the \code{tasks.json}
config file as the simplest data type required to represent all output
type ID values across all output types in the hub.
When only point estimate output types (where \code{output_type_id}s are \code{NA},) are
being collected by a hub, the \code{output_type_id} column is assigned a \code{character}
data type when auto-determined.
Other data type values can be used to override automatic determination.
Note that attempting to coerce \code{output_type_id} to a data type that is
not valid for the data (e.g. trying to coerce\code{"character"} values to
\code{"double"}) will likely result in an error or potentially unexpected
behaviour so use with care.}

\item{file_path}{A character string representing the path to the target data file
relative to the \code{target-data} directory.}

\item{hub_path}{Either a character string path to a local Modeling Hub directory
or an object of class \verb{<SubTreeFileSystem>} created using functions \code{\link[hubData:s3_bucket]{s3_bucket()}}
or \code{\link[hubData:gs_bucket]{gs_bucket()}} by providing a string S3 or GCS bucket name or path to a
Modeling Hub directory stored in the cloud.
For more details consult the
\href{https://arrow.apache.org/docs/r/articles/fs.html}{Using cloud storage (S3, GCS)}
in the \code{arrow} package.
The hub must be fully configured with valid \code{admin.json} and \code{tasks.json}
files within the \code{hub-config} directory.}
}
\value{
Depending on whether validation has succeeded, one of:
\itemize{
\item \verb{<message/check_success>} condition class object.
\item \verb{<error/check_error>} condition class object.
}

Returned object also inherits from subclass \verb{<hub_check>}.
}
\description{
Check that a target data file has the correct column types according to
target type
}
\details{
Column type validation depends on whether a \code{target-data.json} configuration
file is provided:

\strong{With \code{target-data.json} config:}
Expected column types are determined directly from the schema defined in
the configuration. Validation is performed against the schema specifications
in \code{target-data.json}.

\strong{Without \code{target-data.json} config:}
Expected column types are determined from the dataset itself and validated
for internal consistency across files, which mainly applies to partitioned
datasets.
}
